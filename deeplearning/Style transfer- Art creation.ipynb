{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*\tIn neural style transfer, the parameters are already set (pre-trained).\n",
    "* When we are talking about style transfer the parameters are already set, in particular, they are the parameters of a pre-trained neural network like VGG. \n",
    "* This is why the idea of transfer learning, pretraining and making use of previous models becomes our friend. So in style transfer w or theta if you want to call it is not found. It has already been found via some other tasks like doing classification on imagenet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artistic style transfer (aka neural style transfer)\n",
    "* enables us to transform ordinary images into masterpieces. \n",
    "* what this is is that it is a combination of some deep learning techniques such as convolutional neural networks, transfer learning and auto-encoders.\n",
    "* It's  theoretical background is somewhat hard to grasp and so it implementations can be a little bit complex to understand. In this lecture, we will tackle both the background of style transfer and apply it from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* this technique of deep learning is not a typical neural networks operation. \n",
    "* we know that ypical neural networks tune weights based on input and output pairs. \n",
    "* Here though, we will use pre-trained network and will never update weights. \n",
    "* We will update the  inputs instead.\n",
    "\n",
    " * We shall be using VGG model as a pre-trained network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Pre-trained Model?\n",
    "* well, a pre-trained model is a model created by some one else to solve problem. Instead of building a model from scratch to solve a similar problem, you use the model trained on other problem as a starting point.\n",
    "\n",
    "* A pre-trained model may not be 100% accurate in your application, but it saves huge efforts required to re-invent the wheel. Let me show this to you with a recent example. It is also a time saver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I will be using VGG16 model which is pre-trained on the ImageNet dataset and provided in the keras library for use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Optional parameters:\n",
    "\n",
    "--iter, To specify the number of iterations \\\n",
    "the style transfer takes place (Default is 10)\n",
    "--content_weight, The weight given to the content loss (Default is 0.025)\n",
    "--style_weight, The weight given to the style loss (Default is 1.0)\n",
    "--tv_weight, The weight given to the total variation loss (Default is 1.0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- The total variation loss imposes local spatial continuity between\n",
    "the pixels of the combination image, giving it visual coherence.\n",
    "\n",
    "- The style loss is where the deep learning keeps in --that one is defined\n",
    "using a deep convolutional neural network.\n",
    "Precisely, it consists in a sum of\n",
    "L2 distances between the Gram matrices of the representations of\n",
    "the base image and the style reference image, extracted from\n",
    "different layers of a convnet (trained on ImageNet). \n",
    "\n",
    "The general idea is to capture color/texture information at different spatial\n",
    "scales (fairly large scales --defined by the depth of the layer considered).\n",
    "\n",
    "- The content loss is a L2 distance between the features of the base\n",
    "image (extracted from a deep layer) and the features of the combination image,\n",
    "keeping the generated image close enough to the original one.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
    "import numpy as np\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import time\n",
    "from keras.applications import vgg16\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images\n",
    "* so we are going to transfer the style of an image to another one.\n",
    "* The image we would like to transform is called content image whereas the image we would like to transfer its style is called style image. \n",
    "* Then, style image’s brush strokes would be reflected to content image and this new image is called as generated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_image_path = 'katy_tailor.jpg'\n",
    "style_reference_image_path = 'style.jpg'\n",
    "\n",
    "iterations = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we provide the Content and style images.\n",
    "* You might remember that we must initialize weights randomly in neural networks.\n",
    "* But here, generated image will be initialized randomly instead of weights.\n",
    "* You understand by now that this application is not a typical neural networks.\n",
    "* Let’s construct the code for reading content and style images, generating random image for generated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_nrows = 400; img_ncols = 400\n",
    "# dimensions of the generated picture.\n",
    "# util function to open, resize and format pictures into appropriate tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util function to convert a tensor into a valid image\n",
    "def deprocess_image(x):\n",
    "    x = x.reshape((img_nrows, img_ncols, 3))\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tensor representations of our images\n",
    "base_image = K.variable(preprocess_image(base_image_path))\n",
    "\n",
    "x = K.variable(preprocess_image(base_image_path))\n",
    "style_reference_image = K.variable(preprocess_image(style_reference_image_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We need to work on 2D matrices to calculate gram matrix.\n",
    "* Basically batch flatten command transforms n dimensional matrix to 2 dimensional.\n",
    "* Notice that the structure of VGG network. For istance, size of 3rd convolution layer is (56×56)x256.\n",
    "* Here, 256 refers to number of filters in that layer. If shape of the layer transformed to 256x56x56, 56×56 sized matrices put alongside. Permute dimensions function will help us to organize matrices before flattening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_pixels = np.random.randint(256, size=(img_nrows, img_ncols, 3))\n",
    "combination_image = preprocess_input(np.expand_dims(random_pixels, axis=0))\n",
    "# this will contain our generated image\n",
    "combination_image = K.variable(combination_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Normally, python stores images in 3D numpy array (1 dimension for RGB codes). \n",
    "* However, VGG network designed to work with 4D inputs. If you transfer 3D numpy array to its input, you’ll run into an error called  exception layer “block1_conv1: expected ndim=4, found ndim=3“. \n",
    "* That’s why, we have added expand dimensions command in preprocessing step. This command will add a dummy dimension to handle this fault.\n",
    "* Additionally, our input features we are supplying to the VGG network is 400x400x3. That is why, content, style and generated images are size of 400 by 400."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network\n",
    "* Now, we are going to transfer those images to VGG network as input features.\n",
    "* But, we need outputs of some layers instead of output of network.\n",
    "* Fortunately, Keras offers winning CNN models as out-of-the-box function for us to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_model = vgg16.VGG16(input_tensor=base_image, weights='imagenet', include_top=False)\n",
    "style_model = vgg16.VGG16(input_tensor=style_reference_image, weights='imagenet', include_top=False)\n",
    "\n",
    "# build the VGG16 network with our 3 images as input\n",
    "# the model will be loaded with pre-trained ImageNet weights\n",
    "generated_model = vgg16.VGG16(input_tensor=combination_image, weights='imagenet', include_top=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss\n",
    "* We will store loss value twice, one for content and one for style. In typical neural networks, loss value is calculated by comparing actual output and model output (prediction). \n",
    "* Here, we will compare compressed presentations of auto-encoded images. Please remember that auto-encoded compressed representations are actually outputs of some middle layers. Let’s store each output of a layer and layer name once network is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "content_outputs = dict([(layer.name, layer.output) for layer in content_model.layers])\n",
    "style_outputs = dict([(layer.name, layer.output) for layer in style_model.layers])\n",
    "# build the VGG16 network with our 3 images as input\n",
    "# the model will be loaded with pre-trained ImageNet weights\n",
    "generated_outputs = dict([(layer.name, layer.output) for layer in generated_model.layers])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content loss\n",
    "* We’ll transfer randomly generated image and content image to same VGG network.\n",
    "* Original work uses 5th block’s 2nd convolution layer (block5_conv2) to calculate content loss. \n",
    "* This is not a must, you might use different layer to compress images in your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an auxiliary loss function\n",
    "# designed to maintain the \"content\" of the\n",
    "# base image in the generated image\n",
    "def content_loss(base, combination):\n",
    "    return K.sum(K.square(combination - base))\n",
    "# combine these loss functions into a single scalar\n",
    "loss = K.variable(0)\n",
    "\n",
    "base_image_features = content_outputs['block5_conv2'][0]\n",
    "combination_features = generated_outputs['block5_conv2'][0]\n",
    "contentloss = content_loss(base_image_features, combination_features)\n",
    "\n",
    "feature_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We have already transfer both content and generated images to VGG network in previous step. \n",
    "* We can calculate content loss as squared difference of outputs of same layer for both content and generated one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here, finding distances between gram matrices is expected. Gram matrix can  be calculated by multiplying a matrix with its transposed version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the gram matrix of an image tensor (feature-wise outer product)\n",
    "def gram_matrix(x):\n",
    "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style loss\n",
    "* This loss type is a little bit harder to calculate. Firstly, we will compare first 5 layer’s outputs\n",
    "* Now, we can calculate style loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"style loss\" is designed to maintain\n",
    "# the style of the reference image in the generated image.\n",
    "# It is based on the gram matrices (which capture style) of\n",
    "# feature maps from the style reference image\n",
    "# and from the generated image\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_nrows * img_ncols\n",
    "    return K.sum(K.square(S - C)) / (4. * (pow(channels,2)) * (pow(size,2)))\n",
    "\n",
    "\n",
    "styleloss = K.variable(0)\n",
    "\n",
    "for layer_name in feature_layers:\n",
    "    style_reference_features = style_outputs[layer_name][0]\n",
    "    combination_features = generated_outputs[layer_name][0]\n",
    "    styleloss = styleloss + style_loss(style_reference_features, combination_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total loss\n",
    "* We have calculated both content and style loss. We can calculate total loss right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.025; beta = 0.2\n",
    "loss = alpha * contentloss + beta * styleloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "* Total loss is reflected to all weights backwardly in a back propagation algorithm.\n",
    "* Derivative of total error with respect to the each weight is calculated in neural networks learning procedure.\n",
    "* This calculation is also called as gradient calculation. \n",
    "* In style transfer, we need gradients with respect to the input instead of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the gradients of the generated image wrt the loss\n",
    "grads = K.gradients(loss, combination_image)\n",
    "\n",
    "outputs = [loss]\n",
    "outputs += grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this way, (1, 400, 400, 3) shaped tensor will be calculated as gradients. \n",
    "* Just like our images. Now, we will update input of generated image instead of weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_outputs = K.function([combination_image], outputs)\n",
    "\n",
    "def eval_loss_and_grads(x):\n",
    "    x = x.reshape((1, img_nrows, img_ncols, 3))\n",
    "    outs = f_outputs([x])\n",
    "    loss_value = outs[0]\n",
    "    if len(outs[1:]) == 1:\n",
    "        grad_values = outs[1].flatten().astype('float64')\n",
    "    else:\n",
    "        grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
    "    return loss_value, grad_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this Evaluator class makes it possible\n",
    "# to compute loss and gradients in one pass\n",
    "# while retrieving them via two separate functions,\n",
    "# \"loss\" and \"grads\". This is done because scipy.optimize\n",
    "# requires separate functions for loss and gradients,\n",
    "# but computing them separately would be inefficient.\n",
    "class Evaluator(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "\n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        loss_value, grad_values = eval_loss_and_grads(x)\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0\n",
      "epoch  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
    "# so as to minimize the neural style loss\n",
    "x = preprocess_image(base_image_path)\n",
    "# this will run the network according to the number of iterations set previously, it is said to work\n",
    "#well with 10 iterations, however I am just running it with 2 for now\n",
    "for i in range(0,iterations):\n",
    "    print(\"epoch \",i)\n",
    "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(), fprime=evaluator.grads, maxfun=20)\n",
    "    img = deprocess_image(x.copy())\n",
    "    fname = 'generated_%d.png' % i\n",
    "    save_img(fname, img)# save current generated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
